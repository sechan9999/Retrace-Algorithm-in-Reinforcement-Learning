{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -y python-numpy python-dev cmake zlib1g-dev libjpeg- dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyvirtualdisplay\n",
    "#pip install piglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install piglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cart-Pole Problem\n",
    "\n",
    "Cart-Pole also known as Inverted Pendulum with a center of gravity aobve its pivot point. It is unstable and falls over but can be controlled by moving the cart. The goal of the problem is to keep the pole balanced by moving the cart left or right by applying appropriate forces to the pivot point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym import wrappers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQLearningAgent:\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.2,\n",
    "                 discount_factor=1.0,\n",
    "                 exploration_rate=0.5,\n",
    "                 exploration_decay_rate=0.99):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        ##is discretizing of the state space as the environment is a continous flow of states,\n",
    "        ##we need to discretize the states to make the Q-Table\n",
    "        # Discretize the continuous state space for each of the 4 features.\n",
    "        num_discretization_bins = 7\n",
    "        self._state_bins = [\n",
    "            # Cart position.\n",
    "            self._discretize_range(-2.4, 2.4, num_discretization_bins),\n",
    "            # Cart velocity.\n",
    "            self._discretize_range(-3.0, 3.0, num_discretization_bins),\n",
    "            # Pole angle.\n",
    "            self._discretize_range(-0.5, 0.5, num_discretization_bins),\n",
    "            # Tip velocity.\n",
    "            self._discretize_range(-2.0, 2.0, num_discretization_bins)\n",
    "        ]\n",
    "\n",
    "        # Create a clean Q-Table.\n",
    "        self._num_actions = 2\n",
    "        self._max_bins = max(len(bin) for bin in self._state_bins)\n",
    "        num_states = (self._max_bins + 1) ** len(self._state_bins)\n",
    "        ### Initialize the Q-table by all zeros.\n",
    "        self.q = np.zeros(shape=(num_states, self._num_actions))\n",
    "\n",
    "    @staticmethod\n",
    "    def _discretize_range(lower_bound, upper_bound, num_bins):\n",
    "        return np.linspace(lower_bound, upper_bound, num_bins + 1)[1:-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def _discretize_value(value, bins):\n",
    "        return np.digitize(x=value, bins=bins)\n",
    "\n",
    "    def _build_state(self, observation):\n",
    "        # Discretize the observation features and reduce them to a single integer.\n",
    "        state = sum(\n",
    "            self._discretize_value(feature, self._state_bins[i]) * ((self._max_bins + 1) ** i)\n",
    "            for i, feature in enumerate(observation)\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def begin_episode(self, observation):\n",
    "        # Reduce exploration over time.\n",
    "        self.exploration_rate *= self.exploration_decay_rate\n",
    "\n",
    "        # Get the action for the initial state.\n",
    "        self.state = self._build_state(observation)\n",
    "        return np.argmax(self.q[self.state])\n",
    "\n",
    "    def act(self, observation, reward):\n",
    "        #Travel to the next state (S') as a result of that action (a).\n",
    "        next_state = self._build_state(observation)\n",
    "        ##Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
    "        # Exploration/exploitation: choose a random action or select the best one.\n",
    "        enable_exploration = (1 - self.exploration_rate) <= np.random.uniform(0, 1)\n",
    "        if enable_exploration:\n",
    "            # To explore Explore: select a random action\n",
    "            next_action = np.random.randint(0, self._num_actions)\n",
    "        else:\n",
    "            #Exploit: select the action with max value (future reward)\n",
    "            ##For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "            next_action = np.argmax(self.q[next_state])\n",
    "        # defining  to define the (sub)-probability transition operator given in the section 3.2 of the research paper\n",
    "        aStar = np.argmax(self.q[next_state])\n",
    "        probabilities = [self.exploration_rate / self._num_actions] * self._num_actions\n",
    "        probabilities[aStar] += 1 - self.exploration_rate\n",
    "\n",
    "        exp = 0\n",
    "\n",
    "        for i in range(self._num_actions):\n",
    "        \texp += probabilities[i] * self.q[next_state, i]\n",
    "\n",
    "        # Learn: update Q-Table based on current reward and future action.\n",
    "        #Update Q-table values using the equation (3) in the research paper: \"Safe and Efficient off-policy reinforcement learning\"\n",
    "        self.q[self.state, self.action] += self.learning_rate * \\\n",
    "            (reward + self.discount_factor * exp - self.q[self.state, self.action])\n",
    "        ##Set the next state as the current state.\n",
    "        self.state = next_state\n",
    "        self.action = next_action\n",
    "        return next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeHistory:\n",
    "    def __init__(self,\n",
    "                 capacity,\n",
    "                 plot_episode_count=200,\n",
    "                 max_timesteps_per_episode=200,\n",
    "                 goal_avg_episode_length=195,\n",
    "                 goal_consecutive_episodes=100):\n",
    "\n",
    "        self.lengths = np.zeros(capacity, dtype=int)\n",
    "        self.plot_episode_count = plot_episode_count\n",
    "        self.max_timesteps_per_episode = max_timesteps_per_episode\n",
    "        self.goal_avg_episode_length = goal_avg_episode_length\n",
    "        self.goal_consecutive_episodes = goal_consecutive_episodes\n",
    "\n",
    "        self.point_plot = None\n",
    "        self.mean_plot = None\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "\n",
    "    def __getitem__(self, episode_index):\n",
    "        return self.lengths[episode_index]\n",
    "\n",
    "    def __setitem__(self, episode_index, episode_length):\n",
    "        self.lengths[episode_index] = episode_length\n",
    "    def is_goal_reached(self, episode_index):\n",
    "        avg = np.average(self.lengths[episode_index - self.goal_consecutive_episodes + 1:episode_index + 1])\n",
    "        return avg >= self.goal_avg_episode_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_timestep(index, action, reward, observation):\n",
    "    format_string = \"   \".join([\n",
    "        \"Timestep: {0:3d}\",\n",
    "        \"Action: {1:2d}\",\n",
    "        \"Reward: {2:5.1f}\",\n",
    "        \"Cart Position: {3:6.3f}\",\n",
    "        \"Cart Velocity: {4:6.3f}\",\n",
    "        \"Angle: {5:6.3f}\",\n",
    "        \"Tip Velocity: {6:6.3f}\"\n",
    "    ])\n",
    "    print(format_string.format(index, action, reward, *observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, verbose=False):\n",
    "    max_episodes_to_run = 5000\n",
    "    max_timesteps_per_episode = 200\n",
    "\n",
    "    goal_avg_episode_length = 195\n",
    "    goal_consecutive_episodes = 100\n",
    "\n",
    "    plot_episode_count = 200\n",
    "    plot_redraw_frequency = 10\n",
    "\n",
    "    agent = CartPoleQLearningAgent(\n",
    "        learning_rate=0.05,\n",
    "        discount_factor=0.95,\n",
    "        exploration_rate=0.5,\n",
    "        exploration_decay_rate=0.99\n",
    "    )\n",
    "\n",
    "    episode_history = EpisodeHistory(\n",
    "        capacity=max_episodes_to_run,\n",
    "        plot_episode_count=plot_episode_count,\n",
    "        max_timesteps_per_episode=max_timesteps_per_episode,\n",
    "        goal_avg_episode_length=goal_avg_episode_length,\n",
    "        goal_consecutive_episodes=goal_consecutive_episodes\n",
    "    )\n",
    "   \n",
    "\n",
    "    for episode_index in range(max_episodes_to_run):\n",
    "        observation = env.reset()\n",
    "        action = agent.begin_episode(observation)\n",
    "\n",
    "        for timestep_index in range(max_timesteps_per_episode):\n",
    "            # Perform the action and observe the new state.\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            # Update the display and log the current state.\n",
    "            if verbose:\n",
    "                env.render()\n",
    "                log_timestep(timestep_index, action, reward, observation)\n",
    "\n",
    "            # If the episode has ended prematurely, penalize the agent.\n",
    "            if done and timestep_index < max_timesteps_per_episode - 1:\n",
    "                reward = -max_episodes_to_run\n",
    "\n",
    "            # Get the next action from the learner, given our new state.\n",
    "            action = agent.act(observation, reward)\n",
    "\n",
    "            # Record this episode to the history and check if the goal has been reached.\n",
    "            if done or timestep_index == max_timesteps_per_episode - 1:\n",
    "                print(\"Episode {} finished after {} timesteps.\".format(episode_index + 1, timestep_index + 1))\n",
    "\n",
    "                episode_history[episode_index] = timestep_index + 1\n",
    "                \n",
    "                if episode_history.is_goal_reached(episode_index):\n",
    "                    print()\n",
    "                    print(\"Goal reached after {} episodes!\".format(episode_index + 1))\n",
    "                    return episode_history\n",
    "\n",
    "                break\n",
    "\n",
    "    print(\"Goal not reached after {} episodes.\".format(max_episodes_to_run))\n",
    "    return episode_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history, experiment_dir):\n",
    "    # Save the episode lengths to CSV.\n",
    "    filename = os.path.join(experiment_dir, \"episode_history.csv\")\n",
    "    dataframe = pd.DataFrame(history.lengths, columns=[\"length\"])\n",
    "    dataframe.to_csv(filename, header=True, index_label=\"episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    random_state = 0\n",
    "    experiment_dir = \"cartpole-retrace\"\n",
    "\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    env = wrappers.Monitor(env, experiment_dir, force=True, resume=False)\n",
    "    episode_history = run_agent(env, verbose=False)   # Set verbose=False to greatly speed up the process.\n",
    "    save_history(episode_history, experiment_dir)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 22 timesteps.\n",
      "Episode 2 finished after 13 timesteps.\n",
      "Episode 3 finished after 24 timesteps.\n",
      "Episode 4 finished after 11 timesteps.\n",
      "Episode 5 finished after 19 timesteps.\n",
      "Episode 6 finished after 30 timesteps.\n",
      "Episode 7 finished after 27 timesteps.\n",
      "Episode 8 finished after 34 timesteps.\n",
      "Episode 9 finished after 36 timesteps.\n",
      "Episode 10 finished after 13 timesteps.\n",
      "Episode 11 finished after 42 timesteps.\n",
      "Episode 12 finished after 12 timesteps.\n",
      "Episode 13 finished after 11 timesteps.\n",
      "Episode 14 finished after 39 timesteps.\n",
      "Episode 15 finished after 19 timesteps.\n",
      "Episode 16 finished after 12 timesteps.\n",
      "Episode 17 finished after 17 timesteps.\n",
      "Episode 18 finished after 12 timesteps.\n",
      "Episode 19 finished after 20 timesteps.\n",
      "Episode 20 finished after 16 timesteps.\n",
      "Episode 21 finished after 11 timesteps.\n",
      "Episode 22 finished after 27 timesteps.\n",
      "Episode 23 finished after 24 timesteps.\n",
      "Episode 24 finished after 42 timesteps.\n",
      "Episode 25 finished after 52 timesteps.\n",
      "Episode 26 finished after 9 timesteps.\n",
      "Episode 27 finished after 94 timesteps.\n",
      "Episode 28 finished after 12 timesteps.\n",
      "Episode 29 finished after 13 timesteps.\n",
      "Episode 30 finished after 10 timesteps.\n",
      "Episode 31 finished after 12 timesteps.\n",
      "Episode 32 finished after 130 timesteps.\n",
      "Episode 33 finished after 17 timesteps.\n",
      "Episode 34 finished after 43 timesteps.\n",
      "Episode 35 finished after 84 timesteps.\n",
      "Episode 36 finished after 80 timesteps.\n",
      "Episode 37 finished after 38 timesteps.\n",
      "Episode 38 finished after 66 timesteps.\n",
      "Episode 39 finished after 9 timesteps.\n",
      "Episode 40 finished after 62 timesteps.\n",
      "Episode 41 finished after 66 timesteps.\n",
      "Episode 42 finished after 63 timesteps.\n",
      "Episode 43 finished after 49 timesteps.\n",
      "Episode 44 finished after 85 timesteps.\n",
      "Episode 45 finished after 41 timesteps.\n",
      "Episode 46 finished after 52 timesteps.\n",
      "Episode 47 finished after 14 timesteps.\n",
      "Episode 48 finished after 22 timesteps.\n",
      "Episode 49 finished after 60 timesteps.\n",
      "Episode 50 finished after 77 timesteps.\n",
      "Episode 51 finished after 10 timesteps.\n",
      "Episode 52 finished after 10 timesteps.\n",
      "Episode 53 finished after 98 timesteps.\n",
      "Episode 54 finished after 11 timesteps.\n",
      "Episode 55 finished after 63 timesteps.\n",
      "Episode 56 finished after 19 timesteps.\n",
      "Episode 57 finished after 129 timesteps.\n",
      "Episode 58 finished after 117 timesteps.\n",
      "Episode 59 finished after 50 timesteps.\n",
      "Episode 60 finished after 185 timesteps.\n",
      "Episode 61 finished after 69 timesteps.\n",
      "Episode 62 finished after 124 timesteps.\n",
      "Episode 63 finished after 177 timesteps.\n",
      "Episode 64 finished after 124 timesteps.\n",
      "Episode 65 finished after 137 timesteps.\n",
      "Episode 66 finished after 117 timesteps.\n",
      "Episode 67 finished after 105 timesteps.\n",
      "Episode 68 finished after 77 timesteps.\n",
      "Episode 69 finished after 121 timesteps.\n",
      "Episode 70 finished after 121 timesteps.\n",
      "Episode 71 finished after 65 timesteps.\n",
      "Episode 72 finished after 185 timesteps.\n",
      "Episode 73 finished after 110 timesteps.\n",
      "Episode 74 finished after 111 timesteps.\n",
      "Episode 75 finished after 139 timesteps.\n",
      "Episode 76 finished after 122 timesteps.\n",
      "Episode 77 finished after 156 timesteps.\n",
      "Episode 78 finished after 138 timesteps.\n",
      "Episode 79 finished after 117 timesteps.\n",
      "Episode 80 finished after 149 timesteps.\n",
      "Episode 81 finished after 200 timesteps.\n",
      "Episode 82 finished after 123 timesteps.\n",
      "Episode 83 finished after 200 timesteps.\n",
      "Episode 84 finished after 109 timesteps.\n",
      "Episode 85 finished after 154 timesteps.\n",
      "Episode 86 finished after 136 timesteps.\n",
      "Episode 87 finished after 109 timesteps.\n",
      "Episode 88 finished after 62 timesteps.\n",
      "Episode 89 finished after 200 timesteps.\n",
      "Episode 90 finished after 64 timesteps.\n",
      "Episode 91 finished after 199 timesteps.\n",
      "Episode 92 finished after 85 timesteps.\n",
      "Episode 93 finished after 125 timesteps.\n",
      "Episode 94 finished after 129 timesteps.\n",
      "Episode 95 finished after 88 timesteps.\n",
      "Episode 96 finished after 87 timesteps.\n",
      "Episode 97 finished after 164 timesteps.\n",
      "Episode 98 finished after 125 timesteps.\n",
      "Episode 99 finished after 149 timesteps.\n",
      "Episode 100 finished after 200 timesteps.\n",
      "Episode 101 finished after 184 timesteps.\n",
      "Episode 102 finished after 200 timesteps.\n",
      "Episode 103 finished after 187 timesteps.\n",
      "Episode 104 finished after 138 timesteps.\n",
      "Episode 105 finished after 200 timesteps.\n",
      "Episode 106 finished after 200 timesteps.\n",
      "Episode 107 finished after 143 timesteps.\n",
      "Episode 108 finished after 200 timesteps.\n",
      "Episode 109 finished after 200 timesteps.\n",
      "Episode 110 finished after 198 timesteps.\n",
      "Episode 111 finished after 200 timesteps.\n",
      "Episode 112 finished after 200 timesteps.\n",
      "Episode 113 finished after 200 timesteps.\n",
      "Episode 114 finished after 21 timesteps.\n",
      "Episode 115 finished after 200 timesteps.\n",
      "Episode 116 finished after 200 timesteps.\n",
      "Episode 117 finished after 183 timesteps.\n",
      "Episode 118 finished after 200 timesteps.\n",
      "Episode 119 finished after 200 timesteps.\n",
      "Episode 120 finished after 200 timesteps.\n",
      "Episode 121 finished after 200 timesteps.\n",
      "Episode 122 finished after 20 timesteps.\n",
      "Episode 123 finished after 97 timesteps.\n",
      "Episode 124 finished after 200 timesteps.\n",
      "Episode 125 finished after 163 timesteps.\n",
      "Episode 126 finished after 200 timesteps.\n",
      "Episode 127 finished after 200 timesteps.\n",
      "Episode 128 finished after 200 timesteps.\n",
      "Episode 129 finished after 200 timesteps.\n",
      "Episode 130 finished after 200 timesteps.\n",
      "Episode 131 finished after 200 timesteps.\n",
      "Episode 132 finished after 138 timesteps.\n",
      "Episode 133 finished after 200 timesteps.\n",
      "Episode 134 finished after 200 timesteps.\n",
      "Episode 135 finished after 200 timesteps.\n",
      "Episode 136 finished after 200 timesteps.\n",
      "Episode 137 finished after 110 timesteps.\n",
      "Episode 138 finished after 200 timesteps.\n",
      "Episode 139 finished after 200 timesteps.\n",
      "Episode 140 finished after 200 timesteps.\n",
      "Episode 141 finished after 200 timesteps.\n",
      "Episode 142 finished after 190 timesteps.\n",
      "Episode 143 finished after 200 timesteps.\n",
      "Episode 144 finished after 200 timesteps.\n",
      "Episode 145 finished after 194 timesteps.\n",
      "Episode 146 finished after 139 timesteps.\n",
      "Episode 147 finished after 200 timesteps.\n",
      "Episode 148 finished after 200 timesteps.\n",
      "Episode 149 finished after 200 timesteps.\n",
      "Episode 150 finished after 200 timesteps.\n",
      "Episode 151 finished after 182 timesteps.\n",
      "Episode 152 finished after 200 timesteps.\n",
      "Episode 153 finished after 200 timesteps.\n",
      "Episode 154 finished after 119 timesteps.\n",
      "Episode 155 finished after 115 timesteps.\n",
      "Episode 156 finished after 200 timesteps.\n",
      "Episode 157 finished after 173 timesteps.\n",
      "Episode 158 finished after 180 timesteps.\n",
      "Episode 159 finished after 200 timesteps.\n",
      "Episode 160 finished after 132 timesteps.\n",
      "Episode 161 finished after 200 timesteps.\n",
      "Episode 162 finished after 200 timesteps.\n",
      "Episode 163 finished after 200 timesteps.\n",
      "Episode 164 finished after 135 timesteps.\n",
      "Episode 165 finished after 200 timesteps.\n",
      "Episode 166 finished after 175 timesteps.\n",
      "Episode 167 finished after 200 timesteps.\n",
      "Episode 168 finished after 200 timesteps.\n",
      "Episode 169 finished after 200 timesteps.\n",
      "Episode 170 finished after 200 timesteps.\n",
      "Episode 171 finished after 200 timesteps.\n",
      "Episode 172 finished after 176 timesteps.\n",
      "Episode 173 finished after 200 timesteps.\n",
      "Episode 174 finished after 181 timesteps.\n",
      "Episode 175 finished after 200 timesteps.\n",
      "Episode 176 finished after 200 timesteps.\n",
      "Episode 177 finished after 200 timesteps.\n",
      "Episode 178 finished after 200 timesteps.\n",
      "Episode 179 finished after 200 timesteps.\n",
      "Episode 180 finished after 200 timesteps.\n",
      "Episode 181 finished after 200 timesteps.\n",
      "Episode 182 finished after 200 timesteps.\n",
      "Episode 183 finished after 200 timesteps.\n",
      "Episode 184 finished after 200 timesteps.\n",
      "Episode 185 finished after 200 timesteps.\n",
      "Episode 186 finished after 200 timesteps.\n",
      "Episode 187 finished after 200 timesteps.\n",
      "Episode 188 finished after 200 timesteps.\n",
      "Episode 189 finished after 200 timesteps.\n",
      "Episode 190 finished after 200 timesteps.\n",
      "Episode 191 finished after 200 timesteps.\n",
      "Episode 192 finished after 167 timesteps.\n",
      "Episode 193 finished after 200 timesteps.\n",
      "Episode 194 finished after 200 timesteps.\n",
      "Episode 195 finished after 200 timesteps.\n",
      "Episode 196 finished after 182 timesteps.\n",
      "Episode 197 finished after 200 timesteps.\n",
      "Episode 198 finished after 141 timesteps.\n",
      "Episode 199 finished after 200 timesteps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200 finished after 200 timesteps.\n",
      "Episode 201 finished after 200 timesteps.\n",
      "Episode 202 finished after 200 timesteps.\n",
      "Episode 203 finished after 200 timesteps.\n",
      "Episode 204 finished after 154 timesteps.\n",
      "Episode 205 finished after 200 timesteps.\n",
      "Episode 206 finished after 200 timesteps.\n",
      "Episode 207 finished after 200 timesteps.\n",
      "Episode 208 finished after 200 timesteps.\n",
      "Episode 209 finished after 200 timesteps.\n",
      "Episode 210 finished after 200 timesteps.\n",
      "Episode 211 finished after 200 timesteps.\n",
      "Episode 212 finished after 200 timesteps.\n",
      "Episode 213 finished after 200 timesteps.\n",
      "Episode 214 finished after 200 timesteps.\n",
      "Episode 215 finished after 200 timesteps.\n",
      "Episode 216 finished after 69 timesteps.\n",
      "Episode 217 finished after 200 timesteps.\n",
      "Episode 218 finished after 200 timesteps.\n",
      "Episode 219 finished after 200 timesteps.\n",
      "Episode 220 finished after 200 timesteps.\n",
      "Episode 221 finished after 200 timesteps.\n",
      "Episode 222 finished after 200 timesteps.\n",
      "Episode 223 finished after 190 timesteps.\n",
      "Episode 224 finished after 187 timesteps.\n",
      "Episode 225 finished after 77 timesteps.\n",
      "Episode 226 finished after 129 timesteps.\n",
      "Episode 227 finished after 97 timesteps.\n",
      "Episode 228 finished after 200 timesteps.\n",
      "Episode 229 finished after 200 timesteps.\n",
      "Episode 230 finished after 200 timesteps.\n",
      "Episode 231 finished after 106 timesteps.\n",
      "Episode 232 finished after 199 timesteps.\n",
      "Episode 233 finished after 187 timesteps.\n",
      "Episode 234 finished after 200 timesteps.\n",
      "Episode 235 finished after 200 timesteps.\n",
      "Episode 236 finished after 200 timesteps.\n",
      "Episode 237 finished after 194 timesteps.\n",
      "Episode 238 finished after 200 timesteps.\n",
      "Episode 239 finished after 200 timesteps.\n",
      "Episode 240 finished after 195 timesteps.\n",
      "Episode 241 finished after 174 timesteps.\n",
      "Episode 242 finished after 200 timesteps.\n",
      "Episode 243 finished after 200 timesteps.\n",
      "Episode 244 finished after 200 timesteps.\n",
      "Episode 245 finished after 200 timesteps.\n",
      "Episode 246 finished after 200 timesteps.\n",
      "Episode 247 finished after 200 timesteps.\n",
      "Episode 248 finished after 200 timesteps.\n",
      "Episode 249 finished after 200 timesteps.\n",
      "Episode 250 finished after 200 timesteps.\n",
      "Episode 251 finished after 192 timesteps.\n",
      "Episode 252 finished after 200 timesteps.\n",
      "Episode 253 finished after 200 timesteps.\n",
      "Episode 254 finished after 200 timesteps.\n",
      "Episode 255 finished after 72 timesteps.\n",
      "Episode 256 finished after 200 timesteps.\n",
      "Episode 257 finished after 200 timesteps.\n",
      "Episode 258 finished after 197 timesteps.\n",
      "Episode 259 finished after 200 timesteps.\n",
      "Episode 260 finished after 195 timesteps.\n",
      "Episode 261 finished after 200 timesteps.\n",
      "Episode 262 finished after 200 timesteps.\n",
      "Episode 263 finished after 200 timesteps.\n",
      "Episode 264 finished after 200 timesteps.\n",
      "Episode 265 finished after 200 timesteps.\n",
      "Episode 266 finished after 200 timesteps.\n",
      "Episode 267 finished after 200 timesteps.\n",
      "Episode 268 finished after 200 timesteps.\n",
      "Episode 269 finished after 200 timesteps.\n",
      "Episode 270 finished after 199 timesteps.\n",
      "Episode 271 finished after 200 timesteps.\n",
      "Episode 272 finished after 200 timesteps.\n",
      "Episode 273 finished after 200 timesteps.\n",
      "Episode 274 finished after 192 timesteps.\n",
      "Episode 275 finished after 200 timesteps.\n",
      "Episode 276 finished after 200 timesteps.\n",
      "Episode 277 finished after 200 timesteps.\n",
      "Episode 278 finished after 200 timesteps.\n",
      "Episode 279 finished after 200 timesteps.\n",
      "Episode 280 finished after 200 timesteps.\n",
      "Episode 281 finished after 200 timesteps.\n",
      "Episode 282 finished after 200 timesteps.\n",
      "Episode 283 finished after 200 timesteps.\n",
      "Episode 284 finished after 200 timesteps.\n",
      "Episode 285 finished after 200 timesteps.\n",
      "Episode 286 finished after 200 timesteps.\n",
      "Episode 287 finished after 200 timesteps.\n",
      "Episode 288 finished after 200 timesteps.\n",
      "Episode 289 finished after 200 timesteps.\n",
      "Episode 290 finished after 200 timesteps.\n",
      "Episode 291 finished after 200 timesteps.\n",
      "Episode 292 finished after 200 timesteps.\n",
      "Episode 293 finished after 200 timesteps.\n",
      "Episode 294 finished after 200 timesteps.\n",
      "Episode 295 finished after 200 timesteps.\n",
      "Episode 296 finished after 200 timesteps.\n",
      "Episode 297 finished after 200 timesteps.\n",
      "Episode 298 finished after 200 timesteps.\n",
      "Episode 299 finished after 200 timesteps.\n",
      "Episode 300 finished after 200 timesteps.\n",
      "Episode 301 finished after 200 timesteps.\n",
      "Episode 302 finished after 200 timesteps.\n",
      "Episode 303 finished after 200 timesteps.\n",
      "Episode 304 finished after 200 timesteps.\n",
      "Episode 305 finished after 200 timesteps.\n",
      "Episode 306 finished after 200 timesteps.\n",
      "Episode 307 finished after 200 timesteps.\n",
      "Episode 308 finished after 200 timesteps.\n",
      "Episode 309 finished after 200 timesteps.\n",
      "Episode 310 finished after 200 timesteps.\n",
      "Episode 311 finished after 200 timesteps.\n",
      "Episode 312 finished after 200 timesteps.\n",
      "Episode 313 finished after 200 timesteps.\n",
      "Episode 314 finished after 200 timesteps.\n",
      "Episode 315 finished after 200 timesteps.\n",
      "Episode 316 finished after 200 timesteps.\n",
      "Episode 317 finished after 200 timesteps.\n",
      "Episode 318 finished after 200 timesteps.\n",
      "Episode 319 finished after 200 timesteps.\n",
      "Episode 320 finished after 200 timesteps.\n",
      "Episode 321 finished after 200 timesteps.\n",
      "Episode 322 finished after 200 timesteps.\n",
      "Episode 323 finished after 200 timesteps.\n",
      "Episode 324 finished after 200 timesteps.\n",
      "Episode 325 finished after 200 timesteps.\n",
      "\n",
      "Goal reached after 325 episodes!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal reached after 325 episodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cartpole-retrace/episode_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  length\n",
       "0        0      22\n",
       "1        1      13\n",
       "2        2      24\n",
       "3        3      11\n",
       "4        4      19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
